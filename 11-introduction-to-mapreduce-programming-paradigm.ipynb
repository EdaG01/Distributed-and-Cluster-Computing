{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# <center> Introduction to Hadoop MapReduce </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Reality of working with Big Data\n",
    "\n",
    "- Hundreds or thousands of machines to support big data\n",
    "    - Distribute data for storage (HDFS)\n",
    "    - Parallelize data computation (Hadoop MapReduce)\n",
    "    - Handle failure (HDFS and Hadoop MapReduce)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## MapReduce\n",
    "\n",
    "** What is “map”? **\n",
    "A function/procedure that is applied to every individual elements of a collection/list/array/…\n",
    "```\n",
    "int square(x) { return x*x;}\n",
    "map square [1,2,3,4] -> [1,4,9,16]\n",
    "```\n",
    "** What is “reduce”? **\n",
    "A function/procedure that performs an operation on a list. This operation will “fold/reduce” this list into a single value (or a smaller subset)\n",
    "```\n",
    "reduce ([1,2,3,4]) using sum -> 10\n",
    "reduce ([1,2,3,4]) using multiply -> 24\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Implementation of MapReduce Programming Paradigm in Hadoop MapReduce\n",
    "\n",
    "**Programmers implement:**\n",
    "\n",
    "- Map function: Take in the input data and return a key,value pair\n",
    "- Reduce function: Receive the key,value pairs from the mapper and provide a final output as a reduction operation on the pairs\n",
    "- Optional functions:\n",
    "    - Partition function: determines the distribution of mappers’ key,value pairs to the reducers\n",
    "    - Combine functions: initial reduction on the mappers to reduce network traffics\n",
    "    \n",
    "**The MapReduce Framework handles everything else**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## WordCount: The *Hello, World* of Big Data\n",
    "\n",
    "- Count how many unique words there are in a file/multiple files\n",
    "- Standard parallel programming approach:\n",
    "    - Count number of files\n",
    "    - Set number of processes\n",
    "    - Possibly setting up dynamic workload assignment\n",
    "    - A lot of data transfer\n",
    "    - Significant coding effort\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## MapReduce WordCount Example\n",
    "\n",
    "<img src=\"pictures/11/wordcount01.png\" width=\"700\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## MapReduce WordCount Example\n",
    "\n",
    "<img src=\"pictures/11/wordcount02.png\" width=\"700\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## MapReduce PageRank Example 1\n",
    "\n",
    "<img src=\"pictures/11/pagerank01.png\" width=\"700\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## MapReduce PageRank Example 2\n",
    "\n",
    "<img src=\"pictures/11/pagerank02.png\" width=\"700\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## What is \"everything else\"?\n",
    "\n",
    "- Scheduling\n",
    "- Data distribution\n",
    "- Synchronization\n",
    "- Error and Fault Handling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## The cost of \"everything else\"?\n",
    "\n",
    "- All algorithms must be expressed as a combination of mapping, reducing, combining, and partitioning functions \n",
    "- No control over execution placement of mappers and reducers\n",
    "- No control over life cycle of individual mappers and reducers\n",
    "- Very limited information about which mapper handles which data block\n",
    "- Very limited information about which reducer handles which intermediate key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Additional challenge\n",
    "\n",
    "** Large scale debugging on big data programming is difficult\n",
    "\n",
    "- Functional errors are difficult to follow at large scale\n",
    "- Data-dependent errors are even more difficult to catch and fix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Applications of MapReduce\n",
    "\n",
    "- Text tokenization, indexing, and search\n",
    "    - Web access log stats\n",
    "    - Inverted index construction\n",
    "    - Term-vector per host\n",
    "    - Distributed grep/sort\n",
    "- Graph creation\n",
    "    - Web link-graph reversal (Google’s PageRank)\n",
    "- Data Mining and machine learning\n",
    "    - Document clustering\t\n",
    "    - Machine learning\n",
    "    - Statistical machine translation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> Working with Hadoop MapReduce on Cypress </center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "1 8 cores 6gb RAM , 1hour"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Python Jupyter notebook supports execution of Linux command inside the notebook cells. This is done by adding the **!** to the beginning of the command line. It should be noted that each command begins with a **!** will create a new bash shell and close this cell once the execution is done:\n",
    "- Full path is required\n",
    "- Temporary results and environmental variables will be lost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Currently Loaded Modulefiles:\r\n",
      "  1) anaconda3/4.2.0   3) zeromq/4.1.5\r\n",
      "  2) matlab/2015a      4) hdp/0.1\r\n"
     ]
    }
   ],
   "source": [
    "!module list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usage: hdfs [--config confdir] [--loglevel loglevel] COMMAND\r\n",
      "       where COMMAND is one of:\r\n",
      "  dfs                  run a filesystem command on the file systems supported in Hadoop.\r\n",
      "  classpath            prints the classpath\r\n",
      "  namenode -format     format the DFS filesystem\r\n",
      "  secondarynamenode    run the DFS secondary namenode\r\n",
      "  namenode             run the DFS namenode\r\n",
      "  journalnode          run the DFS journalnode\r\n",
      "  zkfc                 run the ZK Failover Controller daemon\r\n",
      "  datanode             run a DFS datanode\r\n",
      "  dfsadmin             run a DFS admin client\r\n",
      "  envvars              display computed Hadoop environment variables\r\n",
      "  haadmin              run a DFS HA admin client\r\n",
      "  fsck                 run a DFS filesystem checking utility\r\n",
      "  balancer             run a cluster balancing utility\r\n",
      "  jmxget               get JMX exported values from NameNode or DataNode.\r\n",
      "  mover                run a utility to move block replicas across\r\n",
      "                       storage types\r\n",
      "  oiv                  apply the offline fsimage viewer to an fsimage\r\n",
      "  oiv_legacy           apply the offline fsimage viewer to an legacy fsimage\r\n",
      "  oev                  apply the offline edits viewer to an edits file\r\n",
      "  fetchdt              fetch a delegation token from the NameNode\r\n",
      "  getconf              get config values from configuration\r\n",
      "  groups               get the groups which users belong to\r\n",
      "  snapshotDiff         diff two snapshots of a directory or diff the\r\n",
      "                       current directory contents with a snapshot\r\n",
      "  lsSnapshottableDir   list all snapshottable dirs owned by the current user\r\n",
      "\t\t\t\t\t\tUse -help to see options\r\n",
      "  portmap              run a portmap service\r\n",
      "  nfs3                 run an NFS version 3 gateway\r\n",
      "  cacheadmin           configure the HDFS cache\r\n",
      "  crypto               configure HDFS encryption zones\r\n",
      "  storagepolicies      list/get/set block storage policies\r\n",
      "  version              print the version\r\n",
      "\r\n",
      "Most commands print help when invoked w/o parameters.\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usage: hadoop fs [generic options]\n",
      "\t[-appendToFile <localsrc> ... <dst>]\n",
      "\t[-cat [-ignoreCrc] <src> ...]\n",
      "\t[-checksum <src> ...]\n",
      "\t[-chgrp [-R] GROUP PATH...]\n",
      "\t[-chmod [-R] <MODE[,MODE]... | OCTALMODE> PATH...]\n",
      "\t[-chown [-R] [OWNER][:[GROUP]] PATH...]\n",
      "\t[-copyFromLocal [-f] [-p] [-l] <localsrc> ... <dst>]\n",
      "\t[-copyToLocal [-p] [-ignoreCrc] [-crc] <src> ... <localdst>]\n",
      "\t[-count [-q] [-h] [-v] [-t [<storage type>]] [-u] <path> ...]\n",
      "\t[-cp [-f] [-p | -p[topax]] <src> ... <dst>]\n",
      "\t[-createSnapshot <snapshotDir> [<snapshotName>]]\n",
      "\t[-deleteSnapshot <snapshotDir> <snapshotName>]\n",
      "\t[-df [-h] [<path> ...]]\n",
      "\t[-du [-s] [-h] <path> ...]\n",
      "\t[-expunge]\n",
      "\t[-find <path> ... <expression> ...]\n",
      "\t[-get [-p] [-ignoreCrc] [-crc] <src> ... <localdst>]\n",
      "\t[-getfacl [-R] <path>]\n",
      "\t[-getfattr [-R] {-n name | -d} [-e en] <path>]\n",
      "\t[-getmerge [-nl] <src> <localdst>]\n",
      "\t[-help [cmd ...]]\n",
      "\t[-ls [-C] [-d] [-h] [-q] [-R] [-t] [-S] [-r] [-u] [<path> ...]]\n",
      "\t[-mkdir [-p] <path> ...]\n",
      "\t[-moveFromLocal <localsrc> ... <dst>]\n",
      "\t[-moveToLocal <src> <localdst>]\n",
      "\t[-mv <src> ... <dst>]\n",
      "\t[-put [-f] [-p] [-l] <localsrc> ... <dst>]\n",
      "\t[-renameSnapshot <snapshotDir> <oldName> <newName>]\n",
      "\t[-rm [-f] [-r|-R] [-skipTrash] [-safely] <src> ...]\n",
      "\t[-rmdir [--ignore-fail-on-non-empty] <dir> ...]\n",
      "\t[-setfacl [-R] [{-b|-k} {-m|-x <acl_spec>} <path>]|[--set <acl_spec> <path>]]\n",
      "\t[-setfattr {-n name [-v value] | -x name} <path>]\n",
      "\t[-setrep [-R] [-w] <rep> <path> ...]\n",
      "\t[-stat [format] <path> ...]\n",
      "\t[-tail [-f] <file>]\n",
      "\t[-test -[defsz] <path>]\n",
      "\t[-text [-ignoreCrc] <src> ...]\n",
      "\t[-touchz <path> ...]\n",
      "\t[-truncate [-w] <length> <path> ...]\n",
      "\t[-usage [cmd ...]]\n",
      "\n",
      "Generic options supported are\n",
      "-conf <configuration file>     specify an application configuration file\n",
      "-D <property=value>            use value for given property\n",
      "-fs <local|namenode:port>      specify a namenode\n",
      "-jt <local|resourcemanager:port>    specify a ResourceManager\n",
      "-files <comma separated list of files>    specify comma separated files to be copied to the map reduce cluster\n",
      "-libjars <comma separated list of jars>    specify comma separated jar files to include in the classpath.\n",
      "-archives <comma separated list of archives>    specify comma separated archives to be unarchived on the compute machines.\n",
      "\n",
      "The general command line syntax is\n",
      "bin/hadoop command [genericOptions] [commandOptions]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge\n",
    "\n",
    "Create a directory named **intro-to-hadoop** inside your user directory on HDFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!cypress-kinit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!klist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!hdfs dfs -ls ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!hdfs dfs -mkdir intro-to-hadoop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Monitoring Hadoop\n",
    "\n",
    "- Hadoop has a web-based interface\n",
    "- Cypress supports a comprehensive managemen framework called Ambari, open sourced by Hortonworks\n",
    "- dscim003.palmetto.clemson.edu:8080\n",
    "    - user/user\n",
    "- Windows: MobaXTerm has embedded X11 support\n",
    "- Mac: need to install XQuartz\n",
    "    - `ssh -X <username>@palmetto.clemson.edu`\n",
    "    - `ssh -X <computeNode>`\n",
    "- Linux:\n",
    "    - `ssh -X <username>@palmetto.clemson.edu`\n",
    "    - `ssh -X <computeNode>`"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (Anaconda)",
   "language": "python",
   "name": "anaconda_py3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
